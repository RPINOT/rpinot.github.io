[{"authors":["admin"],"categories":null,"content":"I am currently a postdoctoral researcher at École Polytechnique Fédérale de Lausanne. I work with Pr. Rachid Guerraoui, Pr. Anne-Marie Kermarrec and Pr. Carmela Troncoso within the Ecocloud Research Center. From october 2017 to december 2020 I completed my PhD in Computer Science at Université Paris Dauphine-PSL and CEA LIST institute, Université Paris Saclay where I was advised by Pr. Jamal Atif, Dr. Florian Yger, and Dr. Cédric Gouy-Pailler. Prior to that, I also obtained in 2017 a MSc in Theoretical and Applied Statistics from Sorbonne Université (Paris 6).\nMy main line of research is in statistical machine learning with a focus on the security and privacy of machine learning applications. I also like to work on statistical analysis of complex data structures such as graphs.\nIn response to the current Covid-19 crisis, I joined a scientific initiative coordinated by Pr. Jamal Atif and Pr. Olivier Cappé. Learn more about it here.\n","date":1626566400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1626566400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently a postdoctoral researcher at École Polytechnique Fédérale de Lausanne. I work with Pr. Rachid Guerraoui, Pr. Anne-Marie Kermarrec and Pr. Carmela Troncoso within the Ecocloud Research Center. From october 2017 to december 2020 I completed my PhD in Computer Science at Université Paris Dauphine-PSL and CEA LIST institute, Université Paris Saclay where I was advised by Pr. Jamal Atif, Dr. Florian Yger, and Dr. Cédric Gouy-Pailler. Prior to that, I also obtained in 2017 a MSc in Theoretical and Applied Statistics from Sorbonne Université (Paris 6).","tags":null,"title":"Rafael Pinot","type":"authors"},{"authors":["Laurent Meunier","Meyer Scetbon","Rafael Pinot","Jamal Atif","Yann Chevaleyre"],"categories":null,"content":"","date":1626566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626566400,"objectID":"c586e09ef1a0a4fc9599ea7936224337","permalink":"/publication/icml2021/","publishdate":"2021-07-18T00:00:00Z","relpermalink":"/publication/icml2021/","section":"publication","summary":"This paper tackles the problem of adversarial examples from a game theoretic point of view. We study the open question of the existence of mixed Nash equilibria in the zero-sum game formed by the attacker and the classifier. While previous works usually allow only one player to use randomized strategies, we show the necessity of considering randomization for both the classifier and the attacker. We demonstrate that this game has no duality gap, meaning that it always admits approximate Nash equilibria. We also provide the first optimization algorithms to learn a mixture of classifiers that approximately realizes the value of this game, i.e. procedures to build an optimally robust randomized classifier.","tags":null,"title":"Mixed Nash Equilibria in the Adversarial Examples Game","type":"publication"},{"authors":["Arnaud Grivet-Sébert","Rafael Pinot","Martin Zuber","Cédric Gouy-Pailler","Renaud Sirdey"],"categories":null,"content":"","date":1616544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616544000,"objectID":"5e2d9f28e67448f5a180fde5123ef061","permalink":"/publication/ecml2021/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/publication/ecml2021/","section":"publication","summary":"This paper addresses the issue of collaborative deep learning with privacy constraints. Building upon differentially private decentralized semi-supervised learning, we introduce homomorphically encrypted operations to extend the set of threats considered so far. While previous methods relied on the existence of an hypothetical 'trusted' third party, we designed specific aggregation operations in the encrypted domain that allow us to circumvent this assumption. This makes our method practical to real-life scenario where data holders do not trust any third party to process their datasets. Crucially the computational burden of the approach is maintained reasonable, making it suitable to deep learning applications. In order to illustrate the performances of our method, we carried out numerical experiments using image datasets in a classification context.","tags":null,"title":"SPEED: Secure, PrivatE,and Efficient Deep learning","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"5af12620aa09ac4307c47728dc927cc9","permalink":"/publication/phd-thesis/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/publication/phd-thesis/","section":"publication","summary":"This thesis investigates the theory of robust classification under adversarial perturbations (a.k.a. adversarial attacks). An adversarial attack refers to a small (humanly imperceptible) change of an input specifically designed to fool a machine learning model. The vulnerability of state of the art classifiers to these attacks has genuine security implications especially for deep neural networks used in AI driven technologies (e.g. for self driving cars). Besides security issues, this shows how little we know about the worst-case behaviors of models the industry uses daily. Accordingly, it became increasingly important for the machine learning community to understand the nature of this failure mode to mitigate the attacks. One can always build trivial classifiers that will not change decision under adversarial manipulation (e.g. constant classifiers) but this comes at odds with standard accuracy of the model. This raises several questions. Among them, we tackle the following one. Can we build a class of models that ensure both robustness to adversarial attacks and accuracy? We first provide some intuition on the adversarial classification problem by adopting a game theoretical point of view. We present the problem as an infinite zero-sum game where classical results (e.g. Nash or Sion theorems) do not apply. We then demonstrate the non existence of a Nash equilibrium in this game when the classifier and the adversary both use deterministic strategies. This constitutes a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that randomized classifiers outperform deterministic ones in term robustness against realistic adversaries. This gives a clear argument for further studying randomized strategies as a defense against adversarial example attacks. Consequently, we present an analysis of randomized classifiers (i.e. classifiers that output random variables) through the lens of statistical learning theory. To do so, we first define a new notion of robustness for randomized classifiers using probability metrics. This definition boils down to forcing the classifier to be locally Lipschitz. We then devise bounds on the generalization gap of any randomized classifier that respects this new notion of robustness. Finally, we upper- bound the adversarial gap (i.e. the gap between the risk and the worst-case risk under attack) of these randomized classifiers. Finally, we highlight some links between our line of research and another emerging topic in machine learning called differential privacy. Both notions build upon the same theoretical ground (i.e. stability of probability metrics). Therefore, results from one domain can be transferred to the other. Based on this idea, we use the differential privacy literature to design a simple noise injection method. The scheme allows us to build a class of robust randomized classifiers out of a deterministic hypothesis class, making our previous findings applicable to a wide range of machine learning models. Open questions and perspectives for future research conclude this work.","tags":null,"title":"On the impact of randomization on robustness in machine learning","type":"publication"},{"authors":["Alexandre Araujo","Laurent Meunier","Rafael Pinot","Benjamin Negrevergne"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"b102931608867a6b102b07dda78de754","permalink":"/publication/ecml2020/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/publication/ecml2020/","section":"publication","summary":"It has been empirically observed that defense mechanisms designed to protect neural networks against $\\ell_\\infty$ adversarial examples offer poor performance against $\\ell_2$ adversarial examples and vice versa. In this paper we conduct a geometrical analysis that validates this observation. Then, we provide a number of empirical insights to illustrate the effect of this phenomenon in practice. Then, we review some of the existing defense mechanism that attempts to defend against multiple attacks by mixing defense strategies. Thanks to our numerical experiments, we discuss the relevance of this method and state open questions for the adversarial examples community.","tags":null,"title":"Advocating for Multiple Defense Strategies against Adversarial Examples","type":"publication"},{"authors":["Rafael Pinot","Raphael Ettegui","Geovani Rizk","Yann Chevaleyre","Jamal Atif"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"91dabe9929be489e00a80e9848665019","permalink":"/publication/randomizationmatters/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/randomizationmatters/","section":"publication","summary":"Is there a classifier that ensures optimal robustness against all adversarial attacks? This paper answers this question by adopting a game-theoretic point of view. We show that adversarial attacks and defenses form an infinite zero-sum game where classical results (e.g. Sion theorem) do not apply. We demonstrate the non-existence of a Nash equilibrium in our game when the classifier and the Adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that, undermild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a new algorithm for building randomized classifiers that are robust to strong adversarial attacks. Empirical results validate our theoretical analysis, and show that our defense method considerably outperforms Adversarial Training against state-of-the-art attacks.","tags":null,"title":"Randomization matters. How to defend against strong adversarial attacks","type":"publication"},{"authors":["Rafael Pinot","Laurent Meunier","Alexandre Araujo","Hisashi Kashima","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1575763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575763200,"objectID":"a32aab51484d503ea23d68726c1868fa","permalink":"/publication/neurips2019/","publishdate":"2019-12-08T00:00:00Z","relpermalink":"/publication/neurips2019/","section":"publication","summary":"This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we provide the first result relating the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. We support our theoretical claims with a set of experiments.","tags":null,"title":"Theoretical evidence for adversarial robustness through randomization","type":"publication"},{"authors":["Rafael Pinot","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1568937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568937600,"objectID":"7e9e3567b171a86a70cf26f9204a405f","permalink":"/publication/ecml2019/","publishdate":"2019-09-20T00:00:00Z","relpermalink":"/publication/ecml2019/","section":"publication","summary":"This short note highlights some links between two lines of research within the emerging topic of trustworthy machine learning differential privacy and robustness to adversarial examples. By abstracting the definitions of both notions, we show that they build upon the same theoretical ground and hence results obtained so far in one domain can be transferred to the other. More precisely, our analysis is based on two key elements probabilistic mappings (also called randomized algorithms in the differential privacy community), and the Renyi divergence which subsumes a large family of divergences. We first generalize the definition of robustness against adversarial examples to encompass probabilistic mappings. Then we observe that Renyi-differential privacy (a recently proposed generalization of differential privacy) and our definition of robustness share several similarities. We finally discuss how both communities can benefit from this connection to transfer technical tools from one research field to the other.","tags":null,"title":"A unified view on differential privacy and robustness to adversarial examples","type":"publication"},{"authors":["Alexandre Araujo","Laurent Meunier","Rafael Pinot","Benjamin Negrevergne"],"categories":null,"content":"","date":1553472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553472000,"objectID":"b2164b3a896bb4f4b160779a09e169ee","permalink":"/publication/rat-preprint/","publishdate":"2019-03-25T00:00:00Z","relpermalink":"/publication/rat-preprint/","section":"publication","summary":"Since the discovery of adversarial examples in machine learning, researchers have designed several techniques to train neural networks that are robust against different types of attacks (most notably ℓ∞ and ℓ2 based attacks). However, it has been observed that the defense mechanisms designed to protect against one type of attack often offer poor performance against the other. In this paper, we introduce Randomized Adversarial Training (RAT), a technique that is efficient both against ℓ2 and ℓ∞ attacks. To obtain this result, we build upon adversarial training, a technique that is efficient against ℓ∞ attacks, and demonstrate that adding random noise at training and inference time further improves performance against ℓ2 attacks. We then show that RAT is as efficient as adversarial training against ℓ∞ attacks while being robust against strong ℓ2 attacks. Our final comparative experiments demonstrate that RAT outperforms all state-of-the-art approaches against ℓ2 and ℓ∞ attacks.","tags":null,"title":"Robust Neural Networks using Randomized Adversarial Training","type":"publication"},{"authors":["Rafael Pinot","Anne Morvan","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1533427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533427200,"objectID":"c122cce2603758714d60c7e6ed7b0740","permalink":"/publication/uai2018/","publishdate":"2018-08-05T00:00:00Z","relpermalink":"/publication/uai2018/","section":"publication","summary":"In this paper, we present the first differentially private clustering method for arbitrary-shaped node clusters in a graph. This algorithm takes as input only an approximate Minimum Spanning Tree (MST) T released under weight differential privacy constraints from the graph. Then, the underlying nonconvex clustering partition is successfully recovered from cutting optimal cuts on T. As opposed to existing methods, our algorithm is theoretically well-motivated. Experiments support our theoretical findings.","tags":null,"title":"Graph-based Clustering under Differential Privacy","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1509321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509321600,"objectID":"2e42c0b70236bb4e417c85cd8ebdb94f","permalink":"/publication/master-thesis/","publishdate":"2017-10-30T00:00:00Z","relpermalink":"/publication/master-thesis/","section":"publication","summary":"We investigate the problem of nodes clustering under privacy constraints when representing a dataset as a graph. Our contribution is threefold. First we formally define the concept of differential privacy for structured databases such as graphs, and give an alternative definition based on a new neighborhood notion between graphs. This definition is adapted to particular frameworks that can be met in various application fields such as genomics, world wide web, population survey, etc. A thorough theoretical analysis of our algorithm stressing the comparison between our bound and the state of the art theoretical bound is presented. To illustrate and support this result we also perform some experiments comparing the two methods based on simulated graphs. Finally, we propose a theoretically motivated method combining a sanitizing mechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree (MST)-based clustering algorithm. It provides an accurate method for nodes clustering in a graph while keeping the sensitive information contained in the edges weights of the private graph. We provide some theoretical results on the robustness of an almost minimum spanning tree construction for Laplace sanitizing mechanisms. These results exhibit which conditions the graph weights should respect in order to consider that the nodes form well separated clusters both for Laplace and our algorithm as sanitizing mechanism. The method has been experimentally evaluated on simulated data, and preliminary results show the good behavior of the algorithm while identifying well separated clusters.","tags":null,"title":"Minimum spanning tree release under differential privacy constraints","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1505433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505433600,"objectID":"774420bd9a8b9768b9542944d94bca35","permalink":"/publication/jdsecommunication/","publishdate":"2017-09-15T00:00:00Z","relpermalink":"/publication/jdsecommunication/","section":"publication","summary":"We investigate the problem of nodes clustering in a graph representation of a dataset under privacy constraints. Our contribution is twofold. First we formally define the concept of differential privacy for graphs and give an  application setting where nodes clustering under privacy constraints allows for a secure analysis of the experiment. Then we propose a theoretically motivated method combining a sanitizing mechanism (such as Laplace or Gaussian mechanism) with a Minimum Spanning Tree (MST)-based clustering algorithm. It provides an accurate method for nodes clustering in a graph while keeping the sensitive information contained in the edges weights of the graph private. We provide some theoretical results on the robustness of the Kruskal minimum spanning tree construction for both of the sanitizing mechanisms. These results exhibit which conditions the graph's weights should respect in order to consider that the nodes form well separated clusters. The method has been experimentally evaluated on simulated data, and preliminary results show the good behavior of the algorithm while identifying well separated clusters.","tags":null,"title":"Nodes clustering in a graph under differential privacy constraints","type":"publication"}]