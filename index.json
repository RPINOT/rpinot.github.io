[{"authors":["admin"],"categories":null,"content":"I am a junior professor in the department of mathematics at Sorbonne Université (Paris 6). I hold a chair on the mathematical foundation of computer and data science within the LPSM research unit. My main line of research is in statistical machine learning with a focus on the privacy and robustness of machine learning algorithms. Most of my work has a theoretical flavor, but I also like to participate in more applied projects to get deeper understanding of state-of-the-art methods, or simply to better grasp the gap that can exist between the theoretical analysis and the empirical performance of some machine learning algorithms.\nFrom 2021 to 2023, I was a postdoctoral researcher at École Polytechnique Fédérale de Lausanne, where I worked with Pr. Rachid Guerraoui and Pr. Anne-Marie Kermarrec within the Ecocloud Research Center. From 2017 to 2020 I completed my PhD in Computer Science at Université Paris Dauphine-PSL and CEA LIST institute, Université Paris Saclay where I was advised by Pr. Jamal Atif, Dr. Florian Yger, and Dr. Cédric Gouy-Pailler.\n","date":1696809600,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1696809600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a junior professor in the department of mathematics at Sorbonne Université (Paris 6). I hold a chair on the mathematical foundation of computer and data science within the LPSM research unit. My main line of research is in statistical machine learning with a focus on the privacy and robustness of machine learning algorithms. Most of my work has a theoretical flavor, but I also like to participate in more applied projects to get deeper understanding of state-of-the-art methods, or simply to better grasp the gap that can exist between the theoretical analysis and the empirical performance of some machine learning algorithms.","tags":null,"title":"Rafael Pinot","type":"authors"},{"authors":["Rachid Guerraoui","Anne-Marie Kermarrec","Anastasiia Kucherenko","Rafael Pinot","Sasha Voitovych"],"categories":null,"content":"","date":1696809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696809600,"objectID":"3172cbd451eb03f13e15b23c8fb85272","permalink":"/publication/disc2023/","publishdate":"2023-10-09T00:00:00Z","relpermalink":"/publication/disc2023/","section":"publication","summary":"The notion of adversary is a staple of distributed computing. An adversary typically models hostile assumptions about the underlying distributed environment, e.g., a network that can drop messages, an operating system that can delay processes or an attacker that can hack machines. So far, the goal of distributed computing researchers has mainly been to develop a distributed algorithm that can face a given adversary, the abstraction characterizing worst-case scenarios. This paper initiates the study of the somehow opposite approach. Given a distributed algorithm, the adversary is the abstraction we seek to implement. More specifically, we consider the problem of controlling the spread of messages in a large- scale system, conveying the practical motivation of limiting the dissemination of fake news or viruses. Essentially, we assume a general class of gossip protocols, called all-to-all gossip protocols, and devise a practical method to hinder the dissemination. We present the Universal Gossip Fighter (UGF). Just like classical adversaries in distributed computing, UGF can observe the status of a dissemination and decide to stop some processes or delay some messages. The originality of UGF lies in the fact that it is universal, i.e., it applies to any all-to-all gossip protocol. We show that any gossip protocol attacked by UGF ends up exhibiting a quadratic message complexity (in the total number of processes) if it achieves sublinear time of dissemination. We also show that if a gossip protocol aims to achieve a message complexity α times smaller than quadratic, then the time complexity rises exponentially in relation to α. We convey the practical relevance of our theoretical findings by implementing UGF and conducting a set of empirical experiments that confirm some of our results.","tags":null,"title":"On the Inherent Anonymity of Gossiping","type":"publication"},{"authors":["Youssef Allouah","Rachid Guerraoui","Nirupam Gupta","Rafael Pinot","John Stephan"],"categories":null,"content":"","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"c931c9673bff8b91c1c511df3cf4e7e3","permalink":"/publication/icml2023_1/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/publication/icml2023_1/","section":"publication","summary":"The ubiquity of distributed machine learning (ML) in sensitive public domain applications calls for algorithms that protect data privacy, while being robust to faults and adversarial behaviors. Although privacy and robustness have been extensively studied independently in distributed ML, their synthesis remains poorly understood. We present the first tight analysis of the error incurred by any algorithm ensuring robustness against a fraction of adversarial machines, as well as differential privacy (DP) for honest machines' data against any other curious entity. Our analysis exhibits a fundamental trade-off between privacy, robustness, and utility. To prove our lower bound, we consider the case of mean estimation, subject to distributed DP and robustness constraints, and devise reductions to centralized estimation of one-way marginals. We prove our matching upper bound by presenting a new distributed ML algorithm using a high-dimensional robust aggregation rule. The latter amortizes the dependence on the dimension in the error (caused by adversarial workers and DP), while being agnostic to the statistical properties of the data.","tags":null,"title":"On the Privacy-Robustness-Utility Trilemma in Distributed Learning","type":"publication"},{"authors":["Sadegh Farhadkhani","Rachid Guerraoui","Nirupam Gupta","Lê Nguyên Hoang","Rafael Pinot","John Stephan"],"categories":null,"content":"","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690070400,"objectID":"9a4c36ab7166f4d9b3716c807d4e05dd","permalink":"/publication/icml2023_2/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/publication/icml2023_2/","section":"publication","summary":"Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates and nearest-neighbor averaging (NNA) for global mixing, respectively. While MoNNA is rather simple to implement, its analysis has been more challenging and relies on two key elements that may be of independent interest. Specifically, we introduce the mixing criterion of (alpha, lambda)-reduction to analyze the non-linear mixing of non-faulty machines, and present a way to control the tension between the momentum and the model drifts. We validate our theory by experiments on image classification and make our code available at https://github.com/LPD-EPFL/robust-collaborative-learning.","tags":null,"title":"Robust Collaborative Learning with Linear Gradient Overhead","type":"publication"},{"authors":["Youssef Allouah","Sadegh Farhadkhani","Rachid Guerraoui","Nirupam Gupta","Rafael Pinot","John Stephan"],"categories":null,"content":"","date":1681171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681171200,"objectID":"93792ff3d141f6870ddc67c830aa95a5","permalink":"/publication/aistats2023/","publishdate":"2023-04-11T00:00:00Z","relpermalink":"/publication/aistats2023/","section":"publication","summary":"Byzantine machine learning (ML) aims to ensure the resilience of distributed learning algorithms to misbehaving (or Byzantine) machines. Although this problem received significant attention, prior works often assume the data held by the machines to be homogeneous, which is seldom true in practical settings. Data heterogeneity makes Byzantine ML considerably more challenging, since a Byzantine machine can hardly be distinguished from a non-Byzantine outlier. A few solutions have been proposed to tackle this issue, but these provide suboptimal probabilistic guarantees and fare poorly in practice. This paper closes the theoretical gap, achieving optimality and inducing good empirical results. In fact, we show how to automatically adapt existing solutions for (homogeneous) Byzantine ML to the heterogeneous setting through a powerful mechanism, we call nearest neighbor mixing (NNM), which boosts any standard robust distributed gradient descent variant to yield optimal Byzantine resilience under heterogeneity. We obtain similar guarantees (in expectation) by plugging NNM in the distributed stochastic heavy ball method, a practical substitute to distributed gradient descent. We obtain empirical results that significantly outperform state-of-the-art Byzantine ML solutions.","tags":null,"title":"Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity","type":"publication"},{"authors":["Laurent Meunier","Raphael Ettedgui","Rafael Pinot","Yann Chevaleyre","Jamal Atif"],"categories":null,"content":"","date":1662595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662595200,"objectID":"c8a60e911304d7cfd77c33291702cefc","permalink":"/publication/neurips2022/","publishdate":"2022-09-08T00:00:00Z","relpermalink":"/publication/neurips2022/","section":"publication","summary":"In this paper, we study the problem of consistency in the context of adversarial examples. Specifically, we tackle the following question; can surrogate losses still be used as a proxy for minimizing the 0/1 loss in the presence of an adversary that alters the inputs at test-time? Different from the standard classification task, this question cannot be reduced to a point-wise minimization problem, and calibration needs not to be sufficient to ensure consistency. In this paper, we expose some pathological behaviors specific to the adversarial problem, and show that no convex surrogate loss can be consistent or calibrated in this context. It is therefore necessary to design another class of surrogate functions that can be used to solve the adversarial consistency issue. As a first step towards designing such a class, we identify sufficient and necessary conditions for a surrogate loss to be calibrated in both the adversarial and standard settings. Finally, we give some directions for building a class of losses that could be consistent in the adversarial framework.","tags":null,"title":"Towards Consistency in Adversarial Classification","type":"publication"},{"authors":["Rafael Pinot","Laurent Meunier","Florian Yger","Cédric Gouy-Pailler","Yann Chevaleyre","Jamal Atif"],"categories":null,"content":"","date":1659398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659398400,"objectID":"a672fafebb2acfb921861d89a9bed50e","permalink":"/publication/mlj2022/","publishdate":"2022-08-02T00:00:00Z","relpermalink":"/publication/mlj2022/","section":"publication","summary":"This paper investigates the theory of robustness against adversarial attacks. We focus on randomized classifiers (i.e. classifiers that output random variables) and provide a thorough analysis of their behavior through the lens of statistical learning theory and information theory. To this aim, we introduce a new notion of robustness for randomized classifiers, enforcing local Lipschitzness using probability metrics. Equipped with this definition, we make two new contributions. The first one consists in devising a new upper bound on the adversarial generalization gap of randomized classifiers. More precisely, we devise bounds on the generalization gap and the adversarial gap i.e. the gap between the risk and the worst-case risk under attack) of randomized classifiers. The second contribution presents a yet simple but efficient noise injection method to design robust randomized classifiers. We show that our results are applicable to a wide range of machine learning models under mild hypotheses. We further corroborate our findings with experimental results using deep neural networks on standard image datasets, namely CIFAR-10 and CIFAR-100. On these tasks, we manage to design robust models that simultaneously achieve state-of-the-art accuracy (over 0.82 clean accuracy on CIFAR-10) and enjoy guaranteed robust accuracy bounds (0.45 against l2 adversaries with magnitude 0.5 on CIFAR-10).","tags":null,"title":"On the robustness of randomized classifiers to adversarial examples","type":"publication"},{"authors":["Sadegh Farhadkhani","Rachid Guerraoui","Nirupam Gupta","Rafael Pinot","John Stephan"],"categories":null,"content":"","date":1658016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658016000,"objectID":"d796f5e8787859ea3c88d8def202f266","permalink":"/publication/icml2022/","publishdate":"2022-07-17T00:00:00Z","relpermalink":"/publication/icml2022/","section":"publication","summary":"Byzantine resilience emerged as a prominent topic within the distributed machine learning community. Essentially, the goal is to enhance distributed optimization algorithms, such as distributed SGD, in a way that guarantees convergence despite the presence of some misbehaving (a.k.a., Byzantine) workers. Although a myriad of techniques addressing the problem have been proposed, the field arguably rests on fragile foundations. These techniques are hard to prove correct and rely on assumptions that are (a) quite unrealistic, i.e., often violated in practice, and (b) heterogeneous, i.e., making it difficult to compare approaches. We present RESAM (RESilient Averaging of Momentums), a unified framework that makes it simple to establish optimal Byzantine resilience, relying only on standard machine learning assumptions. Our framework is mainly composed of two operators, namely resilient averaging at the server and distributed momentum at the workers. We prove a general theorem stating the convergence of distributed SGD under RESAM. Interestingly, demonstrating and comparing the convergence of many existing techniques become direct corollaries of our theorem, without resorting to stringent assumptions. We also present an empirical evaluation of the practical relevance of RESAM.","tags":null,"title":"Byzantine Machine Learning Made Easy by Resilent Averaging of Momentums","type":"publication"},{"authors":["Anastasiia Gorbunova","Rachid Guerraoui","Anne-Marie Kermarrec","Anastasiia Kucherenko","Rafael Pinot"],"categories":null,"content":"","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"6f91e6efd56fedb3a48e56e8398bb237","permalink":"/publication/ipdps2022/","publishdate":"2022-05-30T00:00:00Z","relpermalink":"/publication/ipdps2022/","section":"publication","summary":"The notion of adversary is a staple of distributed computing. An adversary typically models hostile assumptions about the underlying distributed environment, e.g., a network that can drop messages, an operating system that can delay processes or an attacker that can hack machines. So far, the goal of distributed computing researchers has mainly been to develop a distributed algorithm that can face a given adversary, the abstraction characterizing worst-case scenarios. This paper initiates the study of the somehow opposite approach. Given a distributed algorithm, the adversary is the abstraction we seek to implement. More specifically, we consider the problem of controlling the spread of messages in a large- scale system, conveying the practical motivation of limiting the dissemination of fake news or viruses. Essentially, we assume a general class of gossip protocols, called all-to-all gossip protocols, and devise a practical method to hinder the dissemination. We present the Universal Gossip Fighter (UGF). Just like classical adversaries in distributed computing, UGF can observe the status of a dissemination and decide to stop some processes or delay some messages. The originality of UGF lies in the fact that it is universal, i.e., it applies to any all-to-all gossip protocol. We show that any gossip protocol attacked by UGF ends up exhibiting a quadratic message complexity (in the total number of processes) if it achieves sublinear time of dissemination. We also show that if a gossip protocol aims to achieve a message complexity α times smaller than quadratic, then the time complexity rises exponentially in relation to α. We convey the practical relevance of our theoretical findings by implementing UGF and conducting a set of empirical experiments that confirm some of our results.","tags":null,"title":"The Universal Gossip Fighter","type":"publication"},{"authors":["Rachid Guerraoui","Nirupam Gupta","Rafael Pinot","Sebastien Rouault","John Stephan"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"6c160376113e0024c998ec019dc889f6","permalink":"/publication/podc2021/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/publication/podc2021/","section":"publication","summary":"This paper addresses the problem of combining Byzantine resilience with privacy in machine learning (ML). Specifically, we study whether a distributed implementation of the renowned Stochastic Gradient Descent (SGD) learning algorithm is feasible with both differential privacy (DP) and (α,f)-Byzantine resilience. To the best of our knowledge, this is the first work to tackle this problem from a theoretical point of view. A key finding of our analyses is that the classical approaches to these two (seemingly) orthogonal issues are incompatible. More precisely, we show that a direct composition of these techniques makes the guarantees of the resulting SGD algorithm depend unfavourably upon the number of parameters in the ML model, making the training of large models practically infeasible. We validate our theoretical results through numerical experiments on publicly-available datasets; showing that it is impractical to ensure DP and Byzantine resilience simultaneously.","tags":null,"title":"Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?","type":"publication"},{"authors":["Laurent Meunier","Meyer Scetbon","Rafael Pinot","Jamal Atif","Yann Chevaleyre"],"categories":null,"content":"","date":1626566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626566400,"objectID":"7c52f0916c6fa2a59a979c4e7b1523b3","permalink":"/publication/icml2021/","publishdate":"2021-07-18T00:00:00Z","relpermalink":"/publication/icml2021/","section":"publication","summary":"This paper tackles the problem of adversarial examples from a game theoretic point of view. We study the open question of the existence of mixed Nash equilibria in the zero-sum game formed by the attacker and the classifier. While previous works usually allow only one player to use randomized strategies, we show the necessity of considering randomization for both the classifier and the attacker. We demonstrate that this game has no duality gap, meaning that it always admits approximate Nash equilibria. We also provide the first optimization algorithms to learn a mixture of classifiers that approximately realizes the value of this game, i.e. procedures to build an optimally robust randomized classifier.","tags":null,"title":"Mixed Nash Equilibria in the Adversarial Examples Game","type":"publication"},{"authors":["Arnaud Grivet-Sébert","Rafael Pinot","Martin Zuber","Cédric Gouy-Pailler","Renaud Sirdey"],"categories":null,"content":"","date":1616544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616544000,"objectID":"9062822e867994c5b49d6976b68018aa","permalink":"/publication/ecml2021/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/publication/ecml2021/","section":"publication","summary":"This paper addresses the issue of collaborative deep learning with privacy constraints. Building upon differentially private decentralized semi-supervised learning, we introduce homomorphically encrypted operations to extend the set of threats considered so far. While previous methods relied on the existence of an hypothetical 'trusted' third party, we designed specific aggregation operations in the encrypted domain that allow us to circumvent this assumption. This makes our method practical to real-life scenario where data holders do not trust any third party to process their datasets. Crucially the computational burden of the approach is maintained reasonable, making it suitable to deep learning applications. In order to illustrate the performances of our method, we carried out numerical experiments using image datasets in a classification context.","tags":null,"title":"SPEED: Secure, PrivatE,and Efficient Deep learning","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"4880dc8cf426a743dab19a498a5977fe","permalink":"/publication/phd-thesis/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/publication/phd-thesis/","section":"publication","summary":"This thesis investigates the theory of robust classification under adversarial perturbations (a.k.a. adversarial attacks). An adversarial attack refers to a small (humanly imperceptible) change of an input specifically designed to fool a machine learning model. The vulnerability of state of the art classifiers to these attacks has genuine security implications especially for deep neural networks used in AI driven technologies (e.g. for self driving cars). Besides security issues, this shows how little we know about the worst-case behaviors of models the industry uses daily. Accordingly, it became increasingly important for the machine learning community to understand the nature of this failure mode to mitigate the attacks. One can always build trivial classifiers that will not change decision under adversarial manipulation (e.g. constant classifiers) but this comes at odds with standard accuracy of the model. This raises several questions. Among them, we tackle the following one. Can we build a class of models that ensure both robustness to adversarial attacks and accuracy? We first provide some intuition on the adversarial classification problem by adopting a game theoretical point of view. We present the problem as an infinite zero-sum game where classical results (e.g. Nash or Sion theorems) do not apply. We then demonstrate the non existence of a Nash equilibrium in this game when the classifier and the adversary both use deterministic strategies. This constitutes a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that randomized classifiers outperform deterministic ones in term robustness against realistic adversaries. This gives a clear argument for further studying randomized strategies as a defense against adversarial example attacks. Consequently, we present an analysis of randomized classifiers (i.e. classifiers that output random variables) through the lens of statistical learning theory. To do so, we first define a new notion of robustness for randomized classifiers using probability metrics. This definition boils down to forcing the classifier to be locally Lipschitz. We then devise bounds on the generalization gap of any randomized classifier that respects this new notion of robustness. Finally, we upper- bound the adversarial gap (i.e. the gap between the risk and the worst-case risk under attack) of these randomized classifiers. Finally, we highlight some links between our line of research and another emerging topic in machine learning called differential privacy. Both notions build upon the same theoretical ground (i.e. stability of probability metrics). Therefore, results from one domain can be transferred to the other. Based on this idea, we use the differential privacy literature to design a simple noise injection method. The scheme allows us to build a class of robust randomized classifiers out of a deterministic hypothesis class, making our previous findings applicable to a wide range of machine learning models. Open questions and perspectives for future research conclude this work.","tags":null,"title":"On the impact of randomization on robustness in machine learning","type":"publication"},{"authors":["Alexandre Araujo","Laurent Meunier","Rafael Pinot","Benjamin Negrevergne"],"categories":null,"content":"","date":1600041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600041600,"objectID":"e26662d8c7618e9c7494c5fb27f6926d","permalink":"/publication/ecml2020/","publishdate":"2020-09-14T00:00:00Z","relpermalink":"/publication/ecml2020/","section":"publication","summary":"It has been empirically observed that defense mechanisms designed to protect neural networks against $\\ell_\\infty$ adversarial examples offer poor performance against $\\ell_2$ adversarial examples and vice versa. In this paper we conduct a geometrical analysis that validates this observation. Then, we provide a number of empirical insights to illustrate the effect of this phenomenon in practice. Then, we review some of the existing defense mechanism that attempts to defend against multiple attacks by mixing defense strategies. Thanks to our numerical experiments, we discuss the relevance of this method and state open questions for the adversarial examples community.","tags":null,"title":"Advocating for Multiple Defense Strategies against Adversarial Examples","type":"publication"},{"authors":["Rafael Pinot","Raphael Ettegui","Geovani Rizk","Yann Chevaleyre","Jamal Atif"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"744ac5fff5bea2a9ddbe242a8d9ef710","permalink":"/publication/randomizationmatters/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/randomizationmatters/","section":"publication","summary":"Is there a classifier that ensures optimal robustness against all adversarial attacks? This paper answers this question by adopting a game-theoretic point of view. We show that adversarial attacks and defenses form an infinite zero-sum game where classical results (e.g. Sion theorem) do not apply. We demonstrate the non-existence of a Nash equilibrium in our game when the classifier and the Adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that, undermild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a new algorithm for building randomized classifiers that are robust to strong adversarial attacks. Empirical results validate our theoretical analysis, and show that our defense method considerably outperforms Adversarial Training against state-of-the-art attacks.","tags":null,"title":"Randomization matters. How to defend against strong adversarial attacks","type":"publication"},{"authors":["Rafael Pinot","Laurent Meunier","Alexandre Araujo","Hisashi Kashima","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1575763200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575763200,"objectID":"c25c072e38741436e5c5272005c66925","permalink":"/publication/neurips2019/","publishdate":"2019-12-08T00:00:00Z","relpermalink":"/publication/neurips2019/","section":"publication","summary":"This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we provide the first result relating the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. We support our theoretical claims with a set of experiments.","tags":null,"title":"Theoretical evidence for adversarial robustness through randomization","type":"publication"},{"authors":["Rafael Pinot","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1568937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568937600,"objectID":"befd6923b13167e6f3af0affdc26dd2a","permalink":"/publication/ecml2019/","publishdate":"2019-09-20T00:00:00Z","relpermalink":"/publication/ecml2019/","section":"publication","summary":"This short note highlights some links between two lines of research within the emerging topic of trustworthy machine learning differential privacy and robustness to adversarial examples. By abstracting the definitions of both notions, we show that they build upon the same theoretical ground and hence results obtained so far in one domain can be transferred to the other. More precisely, our analysis is based on two key elements probabilistic mappings (also called randomized algorithms in the differential privacy community), and the Renyi divergence which subsumes a large family of divergences. We first generalize the definition of robustness against adversarial examples to encompass probabilistic mappings. Then we observe that Renyi-differential privacy (a recently proposed generalization of differential privacy) and our definition of robustness share several similarities. We finally discuss how both communities can benefit from this connection to transfer technical tools from one research field to the other.","tags":null,"title":"A unified view on differential privacy and robustness to adversarial examples","type":"publication"},{"authors":["Alexandre Araujo","Laurent Meunier","Rafael Pinot","Benjamin Negrevergne"],"categories":null,"content":"","date":1553472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553472000,"objectID":"f7ba4d5669595d166d3d0d9cdd3ab796","permalink":"/publication/rat-preprint/","publishdate":"2019-03-25T00:00:00Z","relpermalink":"/publication/rat-preprint/","section":"publication","summary":"Since the discovery of adversarial examples in machine learning, researchers have designed several techniques to train neural networks that are robust against different types of attacks (most notably ℓ∞ and ℓ2 based attacks). However, it has been observed that the defense mechanisms designed to protect against one type of attack often offer poor performance against the other. In this paper, we introduce Randomized Adversarial Training (RAT), a technique that is efficient both against ℓ2 and ℓ∞ attacks. To obtain this result, we build upon adversarial training, a technique that is efficient against ℓ∞ attacks, and demonstrate that adding random noise at training and inference time further improves performance against ℓ2 attacks. We then show that RAT is as efficient as adversarial training against ℓ∞ attacks while being robust against strong ℓ2 attacks. Our final comparative experiments demonstrate that RAT outperforms all state-of-the-art approaches against ℓ2 and ℓ∞ attacks.","tags":null,"title":"Robust Neural Networks using Randomized Adversarial Training","type":"publication"},{"authors":["Rafael Pinot","Anne Morvan","Florian Yger","Cedric Gouy-Pailler","Jamal Atif"],"categories":null,"content":"","date":1533427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533427200,"objectID":"81ab48461492883c09026471f85f77b0","permalink":"/publication/uai2018/","publishdate":"2018-08-05T00:00:00Z","relpermalink":"/publication/uai2018/","section":"publication","summary":"In this paper, we present the first differentially private clustering method for arbitrary-shaped node clusters in a graph. This algorithm takes as input only an approximate Minimum Spanning Tree (MST) T released under weight differential privacy constraints from the graph. Then, the underlying nonconvex clustering partition is successfully recovered from cutting optimal cuts on T. As opposed to existing methods, our algorithm is theoretically well-motivated. Experiments support our theoretical findings.","tags":null,"title":"Graph-based Clustering under Differential Privacy","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1509321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509321600,"objectID":"42f8e1ee2cc0f38d2622b6a2dc868eff","permalink":"/publication/master-thesis/","publishdate":"2017-10-30T00:00:00Z","relpermalink":"/publication/master-thesis/","section":"publication","summary":"We investigate the problem of nodes clustering under privacy constraints when representing a dataset as a graph. Our contribution is threefold. First we formally define the concept of differential privacy for structured databases such as graphs, and give an alternative definition based on a new neighborhood notion between graphs. This definition is adapted to particular frameworks that can be met in various application fields such as genomics, world wide web, population survey, etc. A thorough theoretical analysis of our algorithm stressing the comparison between our bound and the state of the art theoretical bound is presented. To illustrate and support this result we also perform some experiments comparing the two methods based on simulated graphs. Finally, we propose a theoretically motivated method combining a sanitizing mechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree (MST)-based clustering algorithm. It provides an accurate method for nodes clustering in a graph while keeping the sensitive information contained in the edges weights of the private graph. We provide some theoretical results on the robustness of an almost minimum spanning tree construction for Laplace sanitizing mechanisms. These results exhibit which conditions the graph weights should respect in order to consider that the nodes form well separated clusters both for Laplace and our algorithm as sanitizing mechanism. The method has been experimentally evaluated on simulated data, and preliminary results show the good behavior of the algorithm while identifying well separated clusters.","tags":null,"title":"Minimum spanning tree release under differential privacy constraints","type":"publication"},{"authors":["Rafael Pinot"],"categories":null,"content":"","date":1505433600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505433600,"objectID":"bfd8196bff83d7c3ca3d9bd5064b720b","permalink":"/publication/jdsecommunication/","publishdate":"2017-09-15T00:00:00Z","relpermalink":"/publication/jdsecommunication/","section":"publication","summary":"We investigate the problem of nodes clustering in a graph representation of a dataset under privacy constraints. Our contribution is twofold. First we formally define the concept of differential privacy for graphs and give an  application setting where nodes clustering under privacy constraints allows for a secure analysis of the experiment. Then we propose a theoretically motivated method combining a sanitizing mechanism (such as Laplace or Gaussian mechanism) with a Minimum Spanning Tree (MST)-based clustering algorithm. It provides an accurate method for nodes clustering in a graph while keeping the sensitive information contained in the edges weights of the graph private. We provide some theoretical results on the robustness of the Kruskal minimum spanning tree construction for both of the sanitizing mechanisms. These results exhibit which conditions the graph's weights should respect in order to consider that the nodes form well separated clusters. The method has been experimentally evaluated on simulated data, and preliminary results show the good behavior of the algorithm while identifying well separated clusters.","tags":null,"title":"Nodes clustering in a graph under differential privacy constraints","type":"publication"}]